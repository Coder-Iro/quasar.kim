---
layout: post.njk
title: JAX에서 데이터셋 빠르게 불러오기
summary: Pytorch DataLoader를 JAX랑 같이 쓸 때 병목 현상을 해결해보자
tags: blog
---

<section>
  <p>JAX에 관심이 생겨 최근 이것저것 만져보고 있었습니다. 간단한 모델 훈련 코드를 짜서 돌려보니 CPU에서는 성능이 만족스러웠지만 GPU에선 이상하게 속도가 느렸습니다. 원인을 찾아보니 데이터 로딩 과정에서 병목이 발생하는 것을 발견했습니다. 그래서 <bold>어떻게 하면 JAX에서 데이터를 빠르게 불러올 수 있을까?</bold>에 대해서 나름대로 찾아보고 결과를 정리해보려 글을 써봅니다.</p>

  <h2>어떤 도구를 사용할까?</h2>
  <p>일단 JAX는 데이터 로딩과 관련된 도구를 제공하지 않습니다. <a href="https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch">공식 가이드</a>에도 <em>"JAX는 프로그램 변환과 가속기에서 돌아가는 NumPy에 집중합니다"</em>라고 써져 있고, 대신 PyTorch의 유틸리티들을 추천하네요. 사용가능한 후보들을 나열해보면...</p>

  <p><h6>Tensorflow Datasets</h6> - 전에 한번 써보고 다시는 안쓰기로 했습니다. 디버깅이 정말 최악이었습니다.</p>
  <p><h6>Huggingface Datasets</h6> - 아직 JAX를 지원을 안합니다.</p>
  <p><h6>PyTorch 데이터셋 유틸리티</h6> - JAX 공식 가이드에서도 예시로 이걸 사용하고, 사용했을때도 상당히 만족스러웠습니다.</p>
  <p><h6>직접 만들기</h6> - 굳이...?</p>

  <p>
  적고나니 PyTorch가 당연한 선택처럼 보이네요. 그런데 문제는 생각없이 쓰면 성능이 상당히 느려진다는 것입니다. (사실 그게 접니다;;)
  </p>

  <h2>문제점</h2>
  <p>데이터 로딩 성능을 측정하기 위해서 간단한 실험을 계획해보았습니다. 실험은 MNIST 데이터셋에서 데이터를 한번 완전히 로드하는데 걸리는 시간을 측정해보는 것입니다. 모든 코드는 Colab의 GPU 환경을 이용해서 실행해보았습니다.
  </p>
  <p>일단 다음 코드로 MNIST 데이터셋을 준비해주었습니다.</p>
  {% highlight "python" %}
import torch
from torch.utils import data
from torchvision.datasets import MNIST
import numpy as np

def to_np(img):
  return np.asarray(img, dtype=np.float32)

dataset = MNIST('/tmp/mnist', download=True, transform=to_np)
  {% endhighlight %}

  <p>그리고 그냥 pytorch에서 쓰는것처럼 할 때 성능을 측정해보았습니다.</p>
  {% highlight "python" %}
# experiment 0) torch only
%%timeit -n1 -r5
loader = data.DataLoader(dataset, batch_size=32)
for x, y in loader:
  x, y = x.to('cuda:0'), y.to('cuda:0')
  {% endhighlight %}
  <pre>2.83 s ± 310 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)</pre>
  <p>데이터셋을 한번 완전히 불러오는데 약 2.8초가 걸리네요. </p>


  <p>
    위 코드에서 x, y는 모두 PyTorch Tensor형식입니다.
    이걸 JAX의 DeviceArray로 바꿔줘야 합니다.
    가장 간단하고 직관적인 방법은 위 코드에서 <code>tensor.to()</code>대신에 <code>jnp.array()</code>를 사용하는 것일겁니다.
    그런데 문제는 속도가 상당히 느려진다는 것입니다.
  </p>
  {% highlight "python" %}
# experiment 1) to() 대신에 jnp.array
import jax.numpy as jnp

%%timeit -n1 -r5
loader = data.DataLoader(dataset, batch_size=32)
for x, y in loader:
  x, y = jnp.array(x), jnp.array(y)
  {% endhighlight %}
  <pre>9.64 s ± 264 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)</pre>
  <p>속도가 아까의 2초대에서 9초대로 3배 가까히 뛰어버렸습니다. 이렇게 사용하긴 어렵겠죠.</p>

  <h2>해결책을 찾아서...</h2>
  <p>속도를 빠르게 하기 위해서 다양한 방법들을 시도해보았습니다. 일단 위에서 속도가 느린 건 메모리 복사가 원인인 것 같으니 데이터 로더에 <code>pin_memory=True</code>를 줘 봅시다. <a href="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/">이 글</a>에 따르면 이 옵션을 사용하면 텐서를 GPU로 복사하는 과정에서 CPU 메모리 사이의 복사 과정을 없애준다고 하네요.
  </p>

  {% highlight "python" %}
# experiment 2) 메모리 복사가 느린듯 -> pin_memory 사용하면 빨라질까? 
%%timeit -n1 -r5
loader = data.DataLoader(dataset, batch_size=32, pin_memory=True)
for x, y in loader:
  x, y = jnp.array(x), jnp.array(y)
  {% endhighlight %}
  <pre>9.98 s ± 779 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)</pre>

  <p>
    흠... 램에서의 복사가 원인은 아닌듯 합니다. 오히려 조금 느려졌네요.
  </p>

  <p>
    그렇다면 JAX한테 CPU - GPU간 복사를 시키지 말고, PyTorch한테 시킨 다음 GPU에 올라간 텐서를 JAX ndarray로 바꾸는것도 시도해봅시다. 그런데 GPU로 올라간 텐서를 <code>jnp.asarray(tensor)</code>처럼 바꾸려고 하면 오류가 납니다. 그래서 찾아보니 dlpack이라는걸 이용하면 복사 없이 텐서의 형식을 변환할 수 있다고 합니다. PyTorch는 <code>torch.utils.dlpack</code>를 통해서, JAX는 <code>jax.dlpack</code>을 통해서 dlpack API를 지원합니다.
  </p>

    {% highlight "python" %}
# experiment 3) 보낸 다음에 dlpack으로 복사
%%timeit -n1 -r5

def as_jnp_array(t):
  t = t.to('cuda:0')
  dl = torch.utils.dlpack.to_dlpack(t)
  return jax.dlpack.from_dlpack(dl)

loader = data.DataLoader(dataset, batch_size=32)
for x, y in loader:
  x, y = as_jnp_array(x), as_jnp_array(y)
    {% endhighlight %}
    <pre>4.13 s ± 608 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)</pre>

    <p>
        속도가 2배정도 빨라져서 드디어 PyTorch만 썼을때의 성능에 근접하는 결과가 나와줬습니다. 아무래도 dlpack을 사용하는게 정답인것 같습니다.
    </p>

    <p>
        여기에다가 추가적으로 <code>pin_memory</code>도 줘 봅시다.
    </p>

    {% highlight "python" %}
# experiment 4) 3 + pin_memory
%%timeit -n1 -r5

def as_jnp_array(t):
  t = t.to('cuda:0')
  dl = torch.utils.dlpack.to_dlpack(t)
  return jax.dlpack.from_dlpack(dl)

loader = data.DataLoader(dataset, batch_size=32, pin_memory=True)
for x, y in loader:
  x, y = as_jnp_array(x), as_jnp_array(y)
    {% endhighlight %}
    <pre>3.49 s ± 131 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)</pre>
    <p>
        이정도면 거의 병목 현상이 해결된것 같습니다.
    </p>

    <h2>3줄 정리</h2>
    <ol>
        <li>JAX용 데이터 로드 라이브러리로는 PyTorch가 만족스러웠다.</li>
        <li>텐서를 GPU로 보낸 다음 <code>dlpack</code>을 사용하자.</li>
        <li><code>pin_memory=True</code> 옵션도 같이 사용해주자.</li>
    </ol>
</section>